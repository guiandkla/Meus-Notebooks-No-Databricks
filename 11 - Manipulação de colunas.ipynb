{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ef03b0",
   "metadata": {},
   "source": [
    "### # Databricks notebook source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898d92e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/Volumes/workspace/default/arquivos-aula/Anac/V_OCORRENCIA_AMPLA.json\")\n",
    "display(df)\n",
    "\n",
    "\n",
    "orders_df = spark.read.\\\n",
    "    format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"/Volumes/workspace/default/arquivos-aula/Bike Store/orders.csv\")\n",
    "display(orders_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f972de",
   "metadata": {},
   "source": [
    "### # Verificar os tipos de dados aceitos nas colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e72f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema() # nullable = 'true' significa que a coluna aceita valores nulos.\n",
    "\n",
    "### # Ver nome das colunas (forma simples) na vertical\n",
    "\n",
    "df.columns\n",
    "\n",
    "### # Outra forma de ver as columas (em loop)\n",
    "\n",
    "for Loop in df.columns:\n",
    "    print(Loop)\n",
    "\n",
    "### # Selecionar apenas algumas colunas do Data Frame\n",
    "\n",
    "\n",
    "display(df['Tipo_de_Ocorrencia', 'UF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067e928d",
   "metadata": {},
   "source": [
    "### # Criando uma variável com as colunas (lista)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385afb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ColunasSelecionadas = [\n",
    "    'Categoria_da_Aeronave',\n",
    "    'Classificacao_da_Ocorrência',\n",
    "    'Danos_a_Aeronave',\n",
    "    'Data_da_Ocorrencia',\n",
    "    'Descricao_do_Tipo',\n",
    "    'Fase_da_Operacao'\n",
    "]\n",
    "\n",
    "display(df[ColunasSelecionadas])\n",
    "\n",
    "\n",
    "# Também daria \"no mesmo\" fazer assim:\n",
    "\n",
    "#  ColunasSelecionadas = df['Categoria_da_Aeronave',\n",
    "#    'Classificacao_da_Ocorrência',\n",
    "#    'Danos_a_Aeronave',\n",
    "#    'Data_da_Ocorrencia',\n",
    "#    'Descricao_do_Tipo',\n",
    "#    'Fase_da_Operacao'\n",
    "#    ]\n",
    "\n",
    "#  display(ColunasSelecionadas)\n",
    "\n",
    "### # Método select (para selecionar colunas)\n",
    "\n",
    "display(df.select('Danos_a_Aeronave', 'Data_da_Ocorrencia', 'Operador', 'UF'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba7509f",
   "metadata": {},
   "source": [
    "### # Criando novas colunas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc5d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo a leitura do arquivo JSON\n",
    "nDF = spark.read.json(\"/Volumes/workspace/default/arquivos-aula/Anac/V_OCORRENCIA_AMPLA.json\")\n",
    "display(nDF)\n",
    "\n",
    "### # Criando nova coluna no arquivo JSON\n",
    "\n",
    "teste = nDF['Classificacao_da_Ocorrência', 'UF', 'Municipio']\n",
    "\n",
    "from pyspark.sql.functions import concat, lit\n",
    "\n",
    "display(teste.withColumn('Municipio - UF', concat(nDF.Municipio, lit(\" - \"), nDF[\"UF\"])))\n",
    "\n",
    "# withColumn -> Cria uma coluna (caso não exista) ou substitui uma coluna (caso exista) / Nome da coluna.\n",
    "\n",
    "# concat -> Concatena os valores das colunas (junta o valor filtrado 'nDF.Municipio' com o valor do 'lit' e com o outro valor filtrado 'nDF[\"UF\"]').\n",
    "\n",
    "# lit -> Insere um valor 'literal' nas colunas. Por exemplo, se eu alterar o ' - ' para 'city of', entre a cidade e o estado, resultará em 'Curitiba city of PR', sendo replicado para todos os valores das células na coluna recém criada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a939cf",
   "metadata": {},
   "source": [
    "### # Salvando o tratamento acima em um novo Data Frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d60f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit\n",
    "\n",
    "dfTratado = teste.withColumn(\"Municipio - UF\", concat(nDF.Municipio, lit(\" - \"), nDF[\"UF\"]))\n",
    "\n",
    "display(dfTratado)\n",
    "\n",
    "### # Inserindo mais de uma coluna no mesmo comando e deixando tudo em letra minúscula\n",
    "\n",
    "from pyspark.sql.functions import concat, lit, lower, upper\n",
    "\n",
    "novaDfTratada = teste\\\n",
    "    .withColumn(\"Municipio - UF\", concat(nDF.Municipio, lit(\" - \"), nDF[\"UF\"]))\\\n",
    "    .withColumn(\"País\", lit(\"Brasil\"))\\\n",
    "    .withColumn(\"minúsculo\", lower(teste.Municipio))\n",
    "\n",
    "# .withColumn(\"Municipio - UF\", concat(nDF.Municipio, lit(\" - \"), nDF[\"UF]\") -> Cria uma nova coluna chamada 'Municipio - UF' e concatena os valores das colunas 'Municipio' e 'UF' inserindo o literal ' - ' (que pode ser substituído por qualquer outro nome).\n",
    "\n",
    "# .withColumn(\"País\", lit(\"Brasil\")) -> Cria uma nova coluna chamada país e insere o valor Brasil para todas as células (linhas).\n",
    "\n",
    "# .withColumn(\"minúsculo\"), lower(teste.Municipio) -> Cria uma nova coluna chamada 'minúsculo' e converte todos os valores da coluna para minúsculo.\n",
    "\n",
    "display(novaDfTratada)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895f54b8",
   "metadata": {},
   "source": [
    "### # Criando uma nova coluna chamada 'Regiao' e populando-a de acordo com as células da coluna 'UF'. Caso UF seja igual 'PR' as células na coluna Região receberão o valor 'Sul', senão (se a coluna UF for diferente de 'PR') receberão o valor 'Outra região do Brasil.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd93c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# When -> Quando (semelhante ao 'if' do Python).\n",
    "# Otherwise -> Senão (semelhante ao 'else' do Python).\n",
    "\n",
    "\n",
    "Regiao = novaDfTratada.withColumn(\"Regiao\", when(novaDfTratada.UF == \"PR\", \"Sul\").otherwise(\"Outra região do Brasil\"))\n",
    "display(Regiao)\n",
    "\n",
    "### # Incluíndo mais de uma condição (mais de uma comparação de valor):\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# ISIN (isin) -> Verifica se o valor é um dos valores passados como parâmetro e retorna outro valor. Exemplo: Verifica se na coluna UF possue o valor PR, SC ou RS. Se sim (se corresponde com um dos valores informados como parâmetro, será incluso na nova coluna chamada região o valor 'Região Sul do Brasil'. Senão (caso não corresponda com um dos valores informados como parâmetro) retorna 'Outras regiões do Brasil' por conta do 'otherwise'. A difereça está na comparação simultânea com mais de um único valor. Ao invés de somente 'UF == PR', com 'isin' é possível verificar a correspondência com outros valores simultaneamente.\n",
    "\n",
    "Regiao = Regiao.withColumn(\"Regiao\",\\\n",
    "     when(col(\"UF\").isin(\"PR\", \"SC\", \"RS\"), \"Sul\")\n",
    "    .when(col(\"UF\").isin(\"SP\", \"RJ\", \"MG\", \"ES\"), \"Sudeste\")\n",
    "    .otherwise(\"Outras regiões do Brasil\")\n",
    ")\n",
    "\n",
    "display(Regiao)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
